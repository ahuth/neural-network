{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n",
    "Let's try to \"learn\" the \"exclusive or\" function. Unlike \"and\" or \"or\", it cannot be learned by a single neuron/perceptron, because it is not linearly seperable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For two inputs, xor is true when one of them is true, and false otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_inputs = np.array([\n",
    "    [1, 1],\n",
    "    [1, 0],\n",
    "    [0, 1],\n",
    "    [0, 0]\n",
    "])\n",
    "\n",
    "training_labels = np.array([0, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Massage our training data into the shape we need. This is because `np.dot` needs things to be aligned correctly (in terms of matrix/vector dimensions).\n",
    "\n",
    "Wouldn't need this at all if we were looping through each node of each layer one at a time, because we would just be doing multiplication and not using the dot product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_inputs = training_inputs.T\n",
    "training_labels = training_labels.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example we'll be using the [Relu](https://www.kaggle.com/dansbecker/rectified-linear-units-relu-in-deep-learning) activation function for all layers but the last, for which we'll be using [Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) (so that its values fall between 0 and 1).\n",
    "\n",
    "Note that for back propagation we'll also need the derivatives of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def sigmoid_derivative(Z):\n",
    "    # Note that the derivative of sigmoid is defined in terms of sigmoid itself. This means that we could\n",
    "    # cache off values when we do forward propagation, and re-use them here. Most deep learning applications\n",
    "    # seem to do that. Here I've chosen to keep the overall model simpler, at the expense of computational\n",
    "    # efficiency.\n",
    "    return sigmoid(Z) * (1 - sigmoid(Z))\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    dZ = np.array(Z, copy=True)\n",
    "    \n",
    "    # When Z <= 0, dZ is 0. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    # When Z > 0, dZ is 1.\n",
    "    dZ[Z > 0] = 1\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our main \"model\". This is the highest-level function here, and will handle:\n",
    "- Initializing our weights and biases\n",
    "- Running forward propagation\n",
    "- Running backward propagation\n",
    "- Returning the final weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(inputs, labels, learning_rate, iterations):\n",
    "    # 2 nodes in the input layer, 3 in the hidden layer, and 1 output.\n",
    "    parameters = init_params(2, 3, 1)\n",
    "    costs = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        prediction = propagate_forward(inputs, parameters)\n",
    "        costs.append(compute_cost(prediction, labels))\n",
    "        gradients = propagate_backward(prediction, labels)\n",
    "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "        \n",
    "    # Plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('Cost')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize our weights and biases. There are a couple interesting things to note here.\n",
    "\n",
    "First, the input layer doesn't have weights or biases. So if we have a 3-layer network (input, one hidden, and the output), we end up with only two sets of weights/biases.\n",
    "\n",
    "Second, the number of weights for a layer is the connections between the _previous_ layer and the _current_ one. So if there are 2 inputs and 3 hidden nodes, we have 2 * 3 - or 6 - connections and therefore weights between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(*layer_dimensions):\n",
    "    num_layers = len(layer_dimensions)\n",
    "    weights = []\n",
    "    biases = []\n",
    "    \n",
    "    # As discussed above, we end up with `num_layers - 1` sets of weights and biases.\n",
    "    for l in range(1, num_layers):\n",
    "        # The number of weights for this layer is the product of the current layer's node count and the previous's.\n",
    "        w = np.random.randn(layer_dimensions[l], layer_dimensions[l - 1])\n",
    "        weights.append(w)\n",
    "        \n",
    "        # The number of biases for this layer is just the number of nodes.\n",
    "        b = np.zeros((layer_dimensions[l], 1))\n",
    "        biases.append(b)\n",
    "        \n",
    "    return {\n",
    "        'weights': weights,\n",
    "        'biases': biases\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward propagation.\n",
    "\n",
    "Note that we use Relu as the activation function for all layers except the last one, which we use Sigmoid for.\n",
    "\n",
    "Another interesting thing is that conceptually to calculate the value for a node, we take all of its connections to the previous layer and add up the value of the connected node multiplied by the weight of the connection. While we could easily do that in straight Python, here we're doing all nodes for a given layer at the same time (in parallel) using Numpy.\n",
    "\n",
    "That's why you see a dot product here, instead of looping and multiplying and adding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate_forward(X, parameters):\n",
    "    num_steps = len(parameters['weights'])\n",
    "    # The initial \"activations\" are just the inputs.\n",
    "    activations_current = X\n",
    "    \n",
    "    # Skip the last (output) layer, which we'll handle separately.\n",
    "    for i in range(num_steps - 1):\n",
    "        activations_previous = activations_current\n",
    "        W = parameters['weights'][i]\n",
    "        b = parameters['biases'][i]\n",
    "        activations_current = linear_activation_forward(activations_previous, W, b, relu)\n",
    "        \n",
    "    # For the last layer, use the Sigmoid activation. This is our prediction.\n",
    "    W = parameters['weights'][num_steps - 1]\n",
    "    b = parameters['biases'][num_steps - 1]\n",
    "    prediction = linear_activation_forward(activations_current, W, b, sigmoid)\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "def linear_activation_forward(activations_previous, weights, bias, activation):\n",
    "    # Using the dot product of the weights matrix and the activations vector. This is basically an\n",
    "    # efficient way of adding together the inputs multiplied by each corresponding weight, for all\n",
    "    # nodes in the layer.\n",
    "    Z = np.dot(weights, activations_previous) + bias\n",
    "    A = activation(Z)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the cost. This is a version of [cross-entropy loss](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy), and is used mainly to graph our progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(prediction, labels):\n",
    "    num_examples = labels.shape[1]\n",
    "    \n",
    "    cost = -(1 / num_examples) * np.sum(np.multiply(labels, np.log(prediction)) + np.multiply(1 - labels, np.log(1 - prediction)))\n",
    "    return np.squeeze(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate_backward(prediction, labels):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're finally ready to train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = model(training_inputs, training_labels, 0.25, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And last, let's make some predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inputs, parameters):\n",
    "    num_examples = inputs.shape[1]\n",
    "    num_layers = len(parameters)\n",
    "    predictions = np.zeros(num_examples)\n",
    "    \n",
    "    # Forward propagation\n",
    "    probabilities = propagate_forward(inputs, parameters)\n",
    "    \n",
    "    # Convert probabilities to 0/1\n",
    "    for i in range(0, probabilities.shape[1]):\n",
    "        if probabilities[0, i] > 0.5:\n",
    "            predictions[i] = 1\n",
    "        else:\n",
    "            predictions[i] = 0\n",
    "            \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_on = np.array([\n",
    "    np.array([1, 0]), # True and False\n",
    "    np.array([0, 1]), # False and True\n",
    "    np.array([1, 1]), # True and True\n",
    "    np.array([0, 0]), # False and False\n",
    "    np.array([1, 1]), # True and True\n",
    "    np.array([0, 1]), # False and True\n",
    "    np.array([0, 0]), # False and False\n",
    "    np.array([1, 0])  # True and False\n",
    "])\n",
    "\n",
    "predictions = predict(predict_on.T, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input A</th>\n",
       "      <th>Input B</th>\n",
       "      <th>Predicted XOR</th>\n",
       "      <th>Actual XOR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "dataframe = pd.DataFrame({\n",
    "    'Input A': predict_on.T[0],\n",
    "    'Input B': predict_on.T[1],\n",
    "    'Predicted XOR': predictions,\n",
    "    'Actual XOR': [1, 1, 0, 0, 0, 1, 0, 1]\n",
    "})\n",
    "\n",
    "display(HTML(dataframe.to_html()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
