{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n",
    "Let's try to \"learn\" the \"exclusive or\" function. Unlike \"and\" or \"or\", it cannot be learned by a single neuron/perceptron, because it is not linearly seperable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For two inputs, xor is true when one of them is true, and false otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_inputs = np.array([\n",
    "    [1, 1],\n",
    "    [1, 0],\n",
    "    [0, 1],\n",
    "    [0, 0]\n",
    "])\n",
    "\n",
    "training_labels = np.array([0, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Massage our training data into the shape we need. This is because `np.dot` needs things to be aligned correctly (in terms of matrix/vector dimensions).\n",
    "\n",
    "Wouldn't need this at all if we were looping through each node of each layer one at a time, because we would just be doing multiplication and not using the dot product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_inputs = training_inputs.T\n",
    "training_labels = training_labels.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example we'll be using the [Relu](https://www.kaggle.com/dansbecker/rectified-linear-units-relu-in-deep-learning) activation function for all layers but the last, for which we'll be using [Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) (so that its values fall between 0 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our main \"model\". This is the highest-level function here, and will handle:\n",
    "- Initializing our weights and biases\n",
    "- Running forward propagation\n",
    "- Running backward propagation\n",
    "- Returning the final weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(inputs, labels, learning_rate, iterations):\n",
    "    parameters = init_params(2, 3, 1)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        prediction = propagate_forward(inputs, parameters)\n",
    "        cost = compute_cost(prediction, labels)\n",
    "        gradients = propagate_backward(prediction, labels)\n",
    "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize our weights and biases. There are a couple interesting things to note here.\n",
    "\n",
    "First, the input layer doesn't have weights or biases. So if we have a 3-layer network (input, one hidden, and the output), we end up with only two sets of weights/biases.\n",
    "\n",
    "Second, the number of weights for a layer is the connections between the _previous_ layer and the _current_ one. So if there are 2 inputs and 3 hidden nodes, we have 2 * 3 - or 6 - connections and therefore weights between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(*layer_dimensions):\n",
    "    num_layers = len(layer_dimensions)\n",
    "    weights = []\n",
    "    biases = []\n",
    "    \n",
    "    # As discussed above, we end up with `num_layers - 1` sets of weights and biases.\n",
    "    for l in range(1, num_layers):\n",
    "        # The number of weights for this layer is the product of the current layer's node count and the previous's.\n",
    "        w = np.random.randn(layer_dimensions[l], layer_dimensions[l - 1])\n",
    "        weights.append(w)\n",
    "        \n",
    "        # The number of biases for this layer is just the number of nodes.\n",
    "        b = np.zeros((layer_dimensions[l], 1))\n",
    "        biases.append(b)\n",
    "        \n",
    "    return {\n",
    "        'weights': weights,\n",
    "        'biases': biases\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward propagation.\n",
    "\n",
    "Note that we use Relu as the activation function for all layers except the last one, which we use Sigmoid for.\n",
    "\n",
    "Another interesting thing is that conceptually to calculate the value for a node, we take all of its connections to the previous layer and add up the value of the connected node multiplied by the weight of the connection. While we could easily do that in straight Python, here we're doing all nodes for a given layer at the same time (in parallel) using Numpy.\n",
    "\n",
    "That's why you see a dot product here, instead of looping and multiplying and adding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate_forward(X, parameters):\n",
    "    num_steps = len(parameters['weights'])\n",
    "    # The initial \"activations\" are just the inputs.\n",
    "    activations_current = X\n",
    "    \n",
    "    # Skip the last (output) layer, which we'll handle separately.\n",
    "    for i in range(num_steps - 1):\n",
    "        activations_previous = activations_current\n",
    "        W = parameters['weights'][i]\n",
    "        b = parameters['biases'][i]\n",
    "        activations_current = linear_activation_forward(activations_previous, W, b, relu)\n",
    "        \n",
    "    # For the last layer, use the Sigmoid activation. This is our prediction.\n",
    "    W = parameters['weights'][num_steps - 1]\n",
    "    b = parameters['biases'][num_steps - 1]\n",
    "    prediction = linear_activation_forward(activations_current, W, b, sigmoid)\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "def linear_activation_forward(activations_previous, weights, bias, activation):\n",
    "    # Using the dot product of the weights matrix and the activations vector. This is basically an\n",
    "    # efficient way of adding together the inputs multiplied by each corresponding weight, for all\n",
    "    # nodes in the layer.\n",
    "    Z = np.dot(weights, activations_previous) + bias\n",
    "    A = activation(Z)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(prediction, labels):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate_backward(prediction, labels):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're finally ready to train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = model(training_inputs, training_labels, 0.25, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And last, let's make some predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inputs, parameters):\n",
    "    num_examples = inputs.shape[1]\n",
    "    num_layers = len(parameters)\n",
    "    predictions = np.zeros(num_examples)\n",
    "    \n",
    "    # Forward propagation\n",
    "    probabilities = propagate_forward(inputs, parameters)\n",
    "    \n",
    "    # Convert probabilities to 0/1\n",
    "    for i in range(0, probabilities.shape[1]):\n",
    "        if probabilities[0, i] > 0.5:\n",
    "            predictions[i] = 1\n",
    "        else:\n",
    "            predictions[i] = 0\n",
    "            \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "predict_on = np.array([\n",
    "    np.array([1, 0]), # True and False\n",
    "    np.array([0, 1]), # False and True\n",
    "    np.array([1, 1]), # True and True\n",
    "    np.array([0, 0])  # False and False\n",
    "])\n",
    "\n",
    "predictions = predict(predict_on.T, parameters)\n",
    "\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
